	Adding anaconda3 2022.05 to your environment
	Adding nVidia Cuda Toolkit 11.5
<class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.
<class 'monai.transforms.utility.array.AsChannelFirst'>: Class `AsChannelFirst` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.
wandb: Currently logged in as: barisimre. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.14.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.1
wandb: Run data is saved locally in /local/206166/wandb/run-20230418_191720-2uvqchei
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run MyAttempt1_512
wandb: ‚≠êÔ∏è View project at https://wandb.ai/barisimre/AAA
wandb: üöÄ View run at https://wandb.ai/barisimre/AAA/runs/2uvqchei
Loading dataset:   0%|          | 0/70 [00:00<?, ?it/s]Loading dataset:   1%|‚ñè         | 1/70 [02:03<2:22:05, 123.56s/it]Loading dataset:   3%|‚ñé         | 2/70 [03:54<2:11:42, 116.21s/it]Loading dataset:   4%|‚ñç         | 3/70 [04:04<1:15:33, 67.67s/it] Loading dataset:   6%|‚ñå         | 4/70 [04:05<45:17, 41.17s/it]  Loading dataset:  24%|‚ñà‚ñà‚ñç       | 17/70 [04:49<07:01,  7.95s/it]Loading dataset:  26%|‚ñà‚ñà‚ñå       | 18/70 [04:54<06:39,  7.69s/it]Loading dataset:  31%|‚ñà‚ñà‚ñà‚ñè      | 22/70 [05:04<04:46,  5.96s/it]Loading dataset:  33%|‚ñà‚ñà‚ñà‚ñé      | 23/70 [05:05<04:13,  5.40s/it]Loading dataset:  37%|‚ñà‚ñà‚ñà‚ñã      | 26/70 [05:58<06:47,  9.26s/it]Loading dataset:  39%|‚ñà‚ñà‚ñà‚ñä      | 27/70 [06:15<07:20, 10.24s/it]Loading dataset:  40%|‚ñà‚ñà‚ñà‚ñà      | 28/70 [06:29<07:10, 10.24s/it]Loading dataset:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 29/70 [06:30<06:18,  9.24s/it]Loading dataset:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 33/70 [06:48<04:14,  6.87s/it]Loading dataset:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 34/70 [07:21<06:34, 10.96s/it]Loading dataset:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 38/70 [07:37<04:05,  7.66s/it]Loading dataset:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 41/70 [08:39<05:46, 11.94s/it]Loading dataset:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 44/70 [08:57<04:22, 10.10s/it]Loading dataset:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 45/70 [09:11<04:17, 10.30s/it]Loading dataset:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 46/70 [10:03<06:54, 17.25s/it]Loading dataset:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 52/70 [10:16<02:34,  8.57s/it]Loading dataset:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 57/70 [10:33<01:24,  6.47s/it]Loading dataset:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 59/70 [10:34<00:58,  5.35s/it]Loading dataset:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 61/70 [10:46<00:48,  5.42s/it]Loading dataset:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 62/70 [10:48<00:40,  5.03s/it]Loading dataset:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 65/70 [10:50<00:17,  3.46s/it]Loading dataset:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 66/70 [10:53<00:13,  3.36s/it]Loading dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [10:53<00:00,  9.33s/it]
Loading dataset:   0%|          | 0/10 [00:00<?, ?it/s]Loading dataset:  10%|‚ñà         | 1/10 [01:21<12:14, 81.56s/it]Loading dataset:  20%|‚ñà‚ñà        | 2/10 [01:25<04:45, 35.64s/it]Loading dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:25<00:00,  8.51s/it]
  0%|          | 0/1000 [00:00<?, ?it/s]  0%|          | 0/1000 [07:08<?, ?it/s]
Traceback (most recent call last):
  File "/local/206166/src/main.py", line 46, in <module>
    main()
  File "/local/206166/src/main.py", line 31, in main
    train_single_epoch(model=model, optimizer=optimizer, train_loader=train_loader)
  File "/local/206166/src/training/train.py", line 23, in train_single_epoch
    loss.backward()
  File "/home/s1797743/.conda/envs/my_env/lib/python3.10/site-packages/torch/_tensor.py", line 388, in backward
    return handle_torch_function(
  File "/home/s1797743/.conda/envs/my_env/lib/python3.10/site-packages/torch/overrides.py", line 1498, in handle_torch_function
    result = torch_func_method(public_api, types, args, kwargs)
  File "/home/s1797743/.conda/envs/my_env/lib/python3.10/site-packages/monai/data/meta_tensor.py", line 249, in __torch_function__
    ret = super().__torch_function__(func, types, args, kwargs)
  File "/home/s1797743/.conda/envs/my_env/lib/python3.10/site-packages/torch/_tensor.py", line 1121, in __torch_function__
    ret = func(*args, **kwargs)
  File "/home/s1797743/.conda/envs/my_env/lib/python3.10/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/s1797743/.conda/envs/my_env/lib/python3.10/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [4, 32, 16, 16, 16]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: \ 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: | 0.000 MB of 0.000 MB uploaded (0.000 MB deduped)wandb: / 0.000 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.000 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: \ 0.000 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: | 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: / 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: \ 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: | 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: / 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: Synced MyAttempt1_512: https://wandb.ai/barisimre/AAA/runs/2uvqchei
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230418_191720-2uvqchei/logs
