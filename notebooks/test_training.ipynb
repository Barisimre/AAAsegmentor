{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b33b43b-0dc1-4573-9f0b-807bce5c33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import *\n",
    "from src.model.my_model import MyModel\n",
    "from tqdm import  tqdm\n",
    "from monai.networks import one_hot\n",
    "from src.data.data_loaders import get_loaders\n",
    "from src.utils.metrics import dice_scores\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b4112f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset:   0%|          | 0/70 [00:00<?, ?it/s]IOStream.flush timed out\n",
      "Loading dataset:  51%|█████▏    | 36/70 [03:55<01:54,  3.38s/it]IOStream.flush timed out\n",
      "Loading dataset: 100%|██████████| 70/70 [06:31<00:00,  5.59s/it]\n",
      "Loading dataset:   0%|          | 0/10 [00:00<?, ?it/s]IOStream.flush timed out\n",
      "Loading dataset:   0%|          | 0/10 [00:33<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "applying transform <monai.transforms.utility.dictionary.ToTensord object at 0x7fd880133a90>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/site-packages/monai/transforms/transform.py:102\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# appears where the exception was raised.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/site-packages/monai/transforms/transform.py:66\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, parameters, unpack_parameters)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mparameters)\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/site-packages/monai/transforms/utility/dictionary.py:542\u001b[0m, in \u001b[0;36mToTensord.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_iterator(d):\n\u001b[0;32m--> 542\u001b[0m     d[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush_transform(d, key)\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/site-packages/monai/transforms/utility/array.py:449\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    448\u001b[0m     img\u001b[38;5;241m.\u001b[39mapplied_operations \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# drops tracking info\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrap_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_meta\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/site-packages/monai/utils/type_conversion.py:147\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(data, dtype, device, wrap_sequence, track_meta, safe)\u001b[0m\n\u001b[1;32m    146\u001b[0m     data \u001b[38;5;241m=\u001b[39m safe_dtype_range(data, dtype)\n\u001b[0;32m--> 147\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[43mget_equivalent_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/site-packages/monai/utils/type_conversion.py:81\u001b[0m, in \u001b[0;36mget_equivalent_dtype\u001b[0;34m(dtype, data_type)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dtype\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdtype_numpy_to_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, torch\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# assuming the dtype is ok if it is not a torch dtype and target `data_type` is not torch.Tensor\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/site-packages/monai/utils/type_conversion.py:60\u001b[0m, in \u001b[0;36mdtype_numpy_to_torch\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert a numpy dtype to its torch equivalent.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdtype\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_loaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/final/AAAsegmentor/src/data/data_loaders.py:15\u001b[0m, in \u001b[0;36mget_loaders\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m filenames \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(DATA_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/test_images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m filenames \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m filenames \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmhd\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[0;32m---> 15\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mmonai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCacheDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m monai\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loader, test_loader\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/site-packages/monai/data/dataset.py:814\u001b[0m, in \u001b[0;36mCacheDataset.__init__\u001b[0;34m(self, data, transform, cache_num, cache_rate, num_workers, progress, copy_cache, as_contiguous, hash_as_key, hash_func, runtime_cache)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache: Union[List, ListProxy] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hash_keys: List \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 814\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/site-packages/monai/data/dataset.py:841\u001b[0m, in \u001b[0;36mCacheDataset.set_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    838\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_num))\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# prepare cache content immediately\u001b[39;00m\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache:\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;66;03m# this must be in the main process, not in dataloader's workers\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/site-packages/monai/data/dataset.py:870\u001b[0m, in \u001b[0;36mCacheDataset._fill_cache\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress \u001b[38;5;129;01mand\u001b[39;00m has_tqdm:\n\u001b[0;32m--> 870\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_cache_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoading dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(p\u001b[38;5;241m.\u001b[39mimap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_cache_item, indices))\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/site-packages/monai/data/dataset.py:884\u001b[0m, in \u001b[0;36mCacheDataset._load_cache_item\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     _xform \u001b[38;5;241m=\u001b[39m deepcopy(_transform) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[38;5;28;01melse\u001b[39;00m _transform\n\u001b[0;32m--> 884\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_xform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_contiguous:\n\u001b[1;32m    886\u001b[0m     item \u001b[38;5;241m=\u001b[39m convert_to_contiguous(item, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/site-packages/monai/transforms/transform.py:129\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         _log_stats(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplying transform \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransform\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: applying transform <monai.transforms.utility.dictionary.ToTensord object at 0x7fd880133a90>"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_loaders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47e0eb1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_single_epoch(model, optimizer, train_loader, loss_fn):\n",
    "    losses = []\n",
    "\n",
    "    for d in train_loader:\n",
    "\n",
    "        img = d['img'].to(DEVICE)\n",
    "        mask = d['mask'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(img)\n",
    "\n",
    "        outputs = torch.softmax(outputs, dim=1)\n",
    "        mask = one_hot(mask, num_classes=3)\n",
    "\n",
    "        loss = loss_fn(outputs, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.detach().cpu().item())\n",
    "    print((sum(losses) / len(losses)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941e3d1e-3c21-412d-a81e-0759f8eb667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_epoch(model, test_loader):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "\n",
    "    largest_component = monai.transforms.KeepLargestConnectedComponent()\n",
    "\n",
    "    visualised = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in test_loader:\n",
    "            img = d['img'].to(DEVICE)\n",
    "            mask = d['mask'].to(DEVICE)\n",
    "\n",
    "            out = monai.inferers.sliding_window_inference(img,\n",
    "                                                          roi_size=CROP_SIZE,\n",
    "                                                          sw_batch_size=BATCH_SIZE,\n",
    "                                                          predictor=model,\n",
    "                                                          overlap=0.5,\n",
    "                                                          sw_device=DEVICE,\n",
    "                                                          device=\"cpu\",\n",
    "                                                          progress=False,\n",
    "                                                          )\n",
    "            out = torch.argmax(out, 1, keepdim=True)\n",
    "            out = largest_component(out).to(DEVICE)\n",
    "            s = dice_scores(out, mask)\n",
    "            print(s)\n",
    "            scores.append(s)\n",
    "\n",
    "        scores = np.array(scores)\n",
    "        scores = np.nan_to_num(scores, copy=True, nan=1.0)\n",
    "        scores = np.sum(scores, axis=0) / scores.shape[0]\n",
    "\n",
    "    test_score = np.sum(scores) / (len(scores) * 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd0f9c41-e86a-46ac-9692-7c46ceba276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(in_channels=1, patch_size=PATCH_SIZE, out_channels=3, skip_transformer=False, channels=(32, 32, 32, 32, 32),transformer_channels=16, embed_dim=256).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-4)\n",
    "loss_fn = monai.losses.DiceCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b8744c-c4dc-4041-9973-ef9ce7da3106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "[2023-04-24 13:51:36,749] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-04-24 13:51:36,938] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:51:46,530] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0\n",
      "[2023-04-24 13:51:46,631] torch._inductor.graph: [INFO] Using FallbackKernel: aten.max_pool3d_with_indices\n",
      "[2023-04-24 13:51:47,923] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0\n",
      "[2023-04-24 13:51:47,926] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:51:50,257] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-04-24 13:51:50,291] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __getitem__\n",
      "[2023-04-24 13:51:50,297] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _get_abs_string_index\n",
      "[2023-04-24 13:51:50,300] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing __len__\n",
      "[2023-04-24 13:51:50,306] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-04-24 13:51:50,319] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-04-24 13:51:50,322] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:51:50,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1\n",
      "[2023-04-24 13:51:50,432] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1\n",
      "[2023-04-24 13:51:50,434] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:51:50,443] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:51:50,469] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:51:50,475] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:51:50,479] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:51:51,087] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:51:51,095] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:51:51,097] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:51:51,111] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2\n",
      "[2023-04-24 13:51:51,141] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2\n",
      "[2023-04-24 13:51:51,143] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:51:51,149] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-04-24 13:51:51,157] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-04-24 13:51:51,159] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:51:51,197] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3\n",
      "[2023-04-24 13:51:51,250] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3\n",
      "[2023-04-24 13:51:51,252] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:51:51,258] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:51:51,263] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:51:51,268] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:51:51,272] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:51:51,281] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:51:51,288] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:51:51,290] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:51:51,313] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4\n",
      "[2023-04-24 13:51:51,350] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4\n",
      "[2023-04-24 13:51:51,352] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:51:51,359] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-04-24 13:51:51,367] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-04-24 13:51:51,369] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:51:51,407] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5\n",
      "[2023-04-24 13:51:51,461] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5\n",
      "[2023-04-24 13:51:51,462] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:51:51,469] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:51:51,473] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:51:51,480] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:51:51,484] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:51:51,493] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:51:51,500] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:51:51,502] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:51:51,515] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6\n",
      "[2023-04-24 13:51:51,551] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6\n",
      "[2023-04-24 13:51:51,552] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:51:51,557] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-04-24 13:51:51,565] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-04-24 13:51:51,566] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:51:51,604] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7\n",
      "[2023-04-24 13:51:51,664] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7\n",
      "[2023-04-24 13:51:51,666] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:51:51,672] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:51:51,677] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:51:51,682] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:51:51,686] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:51:51,695] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:51:51,702] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:51:51,704] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:51:51,718] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8\n",
      "[2023-04-24 13:51:51,753] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8\n",
      "[2023-04-24 13:51:51,755] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:51:51,765] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>\n",
      "[2023-04-24 13:51:52,254] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "[2023-04-24 13:51:57,168] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9\n",
      "[2023-04-24 13:51:57,252] torch._inductor.utils: [INFO] using triton random, expect difference from eager\n",
      "[2023-04-24 13:52:01,801] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9\n",
      "[2023-04-24 13:52:01,804] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:01,891] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:52:01,898] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:52:01,905] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:52:01,910] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:52:01,921] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:52:01,928] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:01,930] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:01,947] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10\n",
      "[2023-04-24 13:52:01,953] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10\n",
      "[2023-04-24 13:52:01,954] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:01,960] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:52:01,965] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:52:01,971] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:52:01,977] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:52:01,986] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:52:01,993] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:01,995] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:02,009] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11\n",
      "[2023-04-24 13:52:02,013] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11\n",
      "[2023-04-24 13:52:02,014] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:02,021] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:52:02,026] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:52:02,032] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:52:02,037] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:52:02,047] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:52:02,053] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:02,055] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:02,069] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12\n",
      "[2023-04-24 13:52:02,074] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12\n",
      "[2023-04-24 13:52:02,075] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:02,081] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:52:02,086] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:52:02,093] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:52:02,098] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:52:02,107] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:52:02,114] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:02,116] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:02,130] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13\n",
      "[2023-04-24 13:52:02,134] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13\n",
      "[2023-04-24 13:52:02,136] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:02,143] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>\n",
      "[2023-04-24 13:52:02,161] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:02,163] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:02,225] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14\n",
      "[2023-04-24 13:52:02,337] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14\n",
      "[2023-04-24 13:52:02,339] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:02,360] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>\n",
      "[2023-04-24 13:52:02,517] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:02,522] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:03,543] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15\n",
      "[2023-04-24 13:52:04,586] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15\n",
      "[2023-04-24 13:52:04,589] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:04,666] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 15\n",
      "[2023-04-24 13:52:06,418] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 15\n",
      "[2023-04-24 13:52:06,582] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 14\n",
      "[2023-04-24 13:52:07,364] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 14\n",
      "[2023-04-24 13:52:07,409] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 9\n",
      "[2023-04-24 13:52:11,133] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 9\n",
      "[2023-04-24 13:52:11,246] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 7\n",
      "[2023-04-24 13:52:11,343] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 7\n",
      "[2023-04-24 13:52:11,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 5\n",
      "[2023-04-24 13:52:11,449] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 5\n",
      "[2023-04-24 13:52:11,458] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 3\n",
      "[2023-04-24 13:52:11,554] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 3\n",
      "[2023-04-24 13:52:11,563] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 1\n",
      "[2023-04-24 13:52:11,742] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 1\n",
      "[2023-04-24 13:52:11,766] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 0\n",
      "[2023-04-24 13:52:11,881] torch._inductor.graph: [INFO] Using FallbackKernel: aten.max_pool3d_with_indices_backward\n",
      "[2023-04-24 13:52:13,221] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 0\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "[2023-04-24 13:52:14,914] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "[2023-04-24 13:52:37,908] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-04-24 13:52:38,093] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:39,677] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16\n",
      "[2023-04-24 13:52:39,742] torch._inductor.graph: [INFO] Using FallbackKernel: aten.max_pool3d_with_indices\n",
      "[2023-04-24 13:52:40,727] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16\n",
      "[2023-04-24 13:52:40,729] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:40,780] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-04-24 13:52:40,802] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-04-24 13:52:40,810] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-04-24 13:52:40,812] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:40,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17\n",
      "[2023-04-24 13:52:40,900] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17\n",
      "[2023-04-24 13:52:40,902] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:40,910] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:52:40,914] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:52:40,920] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:52:40,924] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:52:40,933] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:52:40,940] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:40,942] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:40,955] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18\n",
      "[2023-04-24 13:52:40,983] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18\n",
      "[2023-04-24 13:52:40,984] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:40,992] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-04-24 13:52:41,001] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-04-24 13:52:41,003] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:41,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19\n",
      "[2023-04-24 13:52:41,093] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19\n",
      "[2023-04-24 13:52:41,095] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:41,103] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:52:41,108] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:52:41,113] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:52:41,117] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:52:41,127] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:52:41,134] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:41,135] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:41,149] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20\n",
      "[2023-04-24 13:52:41,176] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20\n",
      "[2023-04-24 13:52:41,178] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:41,184] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-04-24 13:52:41,192] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-04-24 13:52:41,194] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:41,234] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21\n",
      "[2023-04-24 13:52:41,287] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21\n",
      "[2023-04-24 13:52:41,289] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:41,297] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:52:41,301] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:52:41,306] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:52:41,311] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:52:41,320] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:52:41,327] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:41,329] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:41,343] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 22\n",
      "[2023-04-24 13:52:41,372] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 22\n",
      "[2023-04-24 13:52:41,375] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:41,381] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-04-24 13:52:41,390] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-04-24 13:52:41,391] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:41,431] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 23\n",
      "[2023-04-24 13:52:41,488] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 23\n",
      "[2023-04-24 13:52:41,489] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:41,497] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:52:41,502] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:52:41,507] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:52:41,512] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:52:41,521] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:52:41,527] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:41,529] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:41,544] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 24\n",
      "[2023-04-24 13:52:41,571] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 24\n",
      "[2023-04-24 13:52:41,572] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:41,583] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>\n",
      "[2023-04-24 13:52:42,075] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:47,194] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 25\n",
      "[2023-04-24 13:52:47,273] torch._inductor.utils: [INFO] using triton random, expect difference from eager\n",
      "[2023-04-24 13:52:52,682] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 25\n",
      "[2023-04-24 13:52:52,685] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:52,736] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:52:52,741] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:52:52,748] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:52:52,753] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:52:52,762] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:52:52,769] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:52,771] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:52,786] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 26\n",
      "[2023-04-24 13:52:52,794] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 26\n",
      "[2023-04-24 13:52:52,796] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:52,801] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:52:52,807] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:52:52,813] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:52:52,817] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:52:52,827] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:52:52,833] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:52,835] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:52,849] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 27\n",
      "[2023-04-24 13:52:52,854] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 27\n",
      "[2023-04-24 13:52:52,855] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:52,861] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:52:52,866] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:52:52,872] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:52:52,878] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:52:52,887] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:52:52,894] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:52,896] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:52,909] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 28\n",
      "[2023-04-24 13:52:52,914] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 28\n",
      "[2023-04-24 13:52:52,916] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:52,921] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing rearrange\n",
      "[2023-04-24 13:52:52,927] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in rearrange>\n",
      "[2023-04-24 13:52:52,933] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing reduce\n",
      "[2023-04-24 13:52:52,938] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing _apply_recipe\n",
      "[2023-04-24 13:52:52,947] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in _apply_recipe>\n",
      "[2023-04-24 13:52:52,954] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in _apply_recipe> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:52,956] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:52,969] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 29\n",
      "[2023-04-24 13:52:52,974] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 29\n",
      "[2023-04-24 13:52:52,975] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:52,995] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>\n",
      "[2023-04-24 13:52:53,013] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:53,015] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:53,078] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 30\n",
      "[2023-04-24 13:52:53,261] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 30\n",
      "[2023-04-24 13:52:53,262] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:52:53,278] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in forward>\n",
      "[2023-04-24 13:52:53,425] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in forward> (RETURN_VALUE)\n",
      "[2023-04-24 13:52:53,430] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-24 13:52:54,489] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 31\n",
      "[2023-04-24 13:52:56,080] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 31\n",
      "[2023-04-24 13:52:56,082] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-24 13:53:07,803] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 31\n",
      "[2023-04-24 13:53:09,523] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 31\n",
      "[2023-04-24 13:53:09,589] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 30\n",
      "[2023-04-24 13:53:10,291] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 30\n",
      "[2023-04-24 13:53:10,340] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 25\n",
      "[2023-04-24 13:53:15,043] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 25\n",
      "[2023-04-24 13:53:15,121] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 23\n",
      "[2023-04-24 13:53:16,080] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 23\n",
      "[2023-04-24 13:53:27,003] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 21\n",
      "[2023-04-24 13:53:27,400] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 21\n",
      "[2023-04-24 13:53:27,409] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 19\n",
      "[2023-04-24 13:53:27,813] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 19\n",
      "[2023-04-24 13:53:27,821] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 17\n",
      "[2023-04-24 13:53:28,000] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 17\n",
      "[2023-04-24 13:53:28,019] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling BACKWARDS graph 16\n",
      "[2023-04-24 13:53:28,096] torch._inductor.graph: [INFO] Using FallbackKernel: aten.max_pool3d_with_indices_backward\n",
      "[2023-04-24 13:53:29,630] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling BACKWARDS graph 16\n",
      "  5%|▌         | 1/20 [01:53<36:04, 113.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2051889101664226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      " 10%|█         | 2/20 [02:18<18:20, 61.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1841529409090679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [02:41<12:28, 44.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1700243420071073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [03:05<09:39, 36.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1496473087204828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [03:29<07:56, 31.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1403683026631672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [03:53<06:45, 28.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1441215011808608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [04:17<05:56, 27.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1437821057107713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [04:40<05:13, 26.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1109963721699185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [05:04<04:38, 25.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.116151147418552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [05:27<04:07, 24.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1148061487409804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [05:49<03:34, 23.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1185422208574083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [06:11<03:04, 23.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1168451640341017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [06:32<02:38, 22.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.087371051311493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [06:53<02:13, 22.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0976895027690463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [07:15<01:49, 21.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1047762698597379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [07:36<01:27, 21.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0886463854047987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [07:58<01:05, 21.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0858200788497925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [08:19<00:43, 21.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.087765759891934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [08:41<00:21, 21.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0923849675390456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [09:02<00:00, 27.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0829876793755426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in tqdm(range(20)):\n",
    "    c_model = train_single_epoch(model, optimizer, train_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec53caba-129e-4782-bae6-3e44ff2898a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94862133 0.11096717 0.01727696]\n",
      "[0.9219479  0.04001863 0.00209648]\n",
      "[0.90451807 0.01981635 0.02504121]\n",
      "[0.94322503 0.11020487 0.05520159]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m, in \u001b[0;36mtest_single_epoch\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m     12\u001b[0m mask \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     14\u001b[0m out \u001b[38;5;241m=\u001b[39m monai\u001b[38;5;241m.\u001b[39minferers\u001b[38;5;241m.\u001b[39msliding_window_inference(img,\n\u001b[1;32m     15\u001b[0m                                               roi_size\u001b[38;5;241m=\u001b[39mCROP_SIZE,\n\u001b[1;32m     16\u001b[0m                                               sw_batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m                                               progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m                                               )\n\u001b[0;32m---> 23\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m out \u001b[38;5;241m=\u001b[39m largest_component(out)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     25\u001b[0m s \u001b[38;5;241m=\u001b[39m dice_scores(out, mask)\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/site-packages/monai/data/meta_tensor.py:268\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 268\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[0;32m~/.conda/envs/final/lib/python3.10/site-packages/torch/_tensor.py:1295\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1295\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1297\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_single_epoch(model=model, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5405613a-455f-4272-9b2d-7cef82709410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
