{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T11:05:36.400238Z",
     "start_time": "2023-04-26T11:05:36.395360Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmy_model\u001b[39;00m \u001b[39mimport\u001b[39;00m MyModel\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbaselines\u001b[39;00m \u001b[39mimport\u001b[39;00m UNet, SWINUNETR\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from src.constants import *\n",
    "from src.model.my_model import MyModel\n",
    "from src.model.baselines import UNet, SWINUNETR\n",
    "import numpy as np\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from src.model.blocks import *\n",
    "import torch.nn as nn\n",
    "from src.utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T11:05:36.661030Z",
     "start_time": "2023-04-26T11:05:36.623276Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "example2 = torch.rand(size=[1, 1, 64, 64, 64]).cuda()\n",
    "mask = torch.rand(size=[1, 3, 128, 128, 128]).cuda()\n",
    "loss_fn = monai.losses.DiceCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T11:05:36.935081Z",
     "start_time": "2023-04-26T11:05:36.684177Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# modes = normal, skip, autoencoder, half_half, no_vit\n",
    "model = MyModel(in_channels=1,\n",
    "                 out_channels=3,\n",
    "                 lower_channels=32,\n",
    "                 big_channel=32,\n",
    "                 patch_size=8,\n",
    "                 embed_dim=512,\n",
    "                 mode=\"normal\",\n",
    "                 old_embedder=True).cuda()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-4)\n",
    "\n",
    "# model = UNet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T11:05:19.159124Z",
     "start_time": "2023-04-26T11:05:18.416592Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 771 ms, sys: 630 ms, total: 1.4 s\n",
      "Wall time: 3.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "out = model(example2)\n",
    "\n",
    "l = loss_fn(out, mask)\n",
    "l.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 10 19:26:15 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.86.01    Driver Version: 515.86.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:3B:00.0 Off |                    0 |\n",
      "|  0%   38C    P8    33W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A40          On   | 00000000:5E:00.0 Off |                    0 |\n",
      "|  0%   61C    P0   201W / 300W |  36234MiB / 46068MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A40          On   | 00000000:B1:00.0 Off |                    0 |\n",
      "|  0%   38C    P8    33W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A40          On   | 00000000:D9:00.0 Off |                    0 |\n",
      "|  0%   64C    P0   152W / 300W |   3877MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A   1233668      C   ...envs/final/bin/python3.10     9869MiB |\n",
      "|    1   N/A  N/A   1655613      C   ...envs/final/bin/python3.10    26363MiB |\n",
      "|    3   N/A  N/A   1641366      C   python                           3875MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"nvidia-smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 128, 128, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122871594"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_learnable_parameters(model.vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41945605"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_learnable_parameters(model.vit.embedders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vit.embedders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_learnable_parameters(SWINUNETR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_learnable_parameters(UNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 16\n",
    "in_channels = 3\n",
    "x_dim, y_dim, z_dim = 128, 128, 128\n",
    "patch_size = 8\n",
    "embedding_dim = 512\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size, in_channels, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.projection = nn.Linear(in_channels * patch_size**3, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, x, y, z)\n",
    "        batch_size, channels, x_dim, y_dim, z_dim = x.shape\n",
    "\n",
    "        # Create non-overlapping patches\n",
    "        patches = x.unfold(2, self.patch_size, self.patch_size)\n",
    "        patches = patches.unfold(3, self.patch_size, self.patch_size)\n",
    "        patches = patches.unfold(4, self.patch_size, self.patch_size)\n",
    "\n",
    "        # Flatten patches\n",
    "        patches_flat = patches.contiguous().view(batch_size, -1, channels * self.patch_size**3)\n",
    "\n",
    "        # Apply linear embeddings\n",
    "        embeddings = self.projection(patches_flat)\n",
    "\n",
    "        return patches_flat, patches.shape\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class InversePatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.projection = nn.Linear(embed_dim, in_channels * patch_size**3)\n",
    "\n",
    "    def forward(self, embeddings, patches_shape):\n",
    "        # Apply inverse linear embeddings\n",
    "        # patches_flat = self.projection(embeddings)\n",
    "\n",
    "        # Reshape patches\n",
    "        print(patches_shape)\n",
    "        batch_size, _, channels, x_unfold, y_unfold, z_unfold = patches_shape\n",
    "        patches = embeddings.view(batch_size, -1, channels, self.patch_size, self.patch_size, self.patch_size)\n",
    "\n",
    "        # Combine non-overlapping patches\n",
    "        x_dim = x_unfold * self.patch_size\n",
    "        y_dim = y_unfold * self.patch_size\n",
    "        z_dim = z_unfold * self.patch_size\n",
    "\n",
    "        patches = patches.view(batch_size, x_unfold, y_unfold, z_unfold, channels, self.patch_size, self.patch_size, self.patch_size)\n",
    "        x = patches.permute(0, 4, 1, 5, 2, 6, 3, 7).reshape(batch_size, channels, x_dim, y_dim, z_dim)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_tensor = torch.randn(batch_size, in_channels, x_dim, y_dim, z_dim)\n",
    "patch_embedding = PatchEmbedding(patch_size, in_channels, embedding_dim)\n",
    "output, patch_shape = patch_embedding(input_tensor)\n",
    "print(output.shape)  # Output: (16, 512, 64)\n",
    "\n",
    "inverse_patch_embedding = InversePatchEmbedding(patch_size, in_channels, embedding_dim)\n",
    "reconstructed_input = inverse_patch_embedding(output, patch_shape)\n",
    "print(reconstructed_input.shape)  # Output: (16, 3, 32, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_tensor == reconstructed_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size, in_channels, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.projection = nn.Linear(in_channels * patch_size**3, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, x_dim, y_dim, z_dim = x.shape\n",
    "\n",
    "        patches_x = x.unfold(2, self.patch_size, self.patch_size)\n",
    "        patches_y = patches_x.unfold(3, self.patch_size, self.patch_size)\n",
    "        patches_z = patches_y.unfold(4, self.patch_size, self.patch_size)\n",
    "\n",
    "        patches = patches_z.permute(0, 2, 3, 4, 1, 5, 6, 7).contiguous()\n",
    "        patches = patches.view(batch_size, -1, self.patch_size**3 * channels)\n",
    "\n",
    "        embeddings = self.projection(patches)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversePatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size, in_channels, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.projection = nn.Linear(embedding_dim, in_channels * patch_size**3)\n",
    "\n",
    "    def forward(self, embeddings, x_shape):\n",
    "        batch_size, _, x_dim, y_dim, z_dim = x_shape\n",
    "\n",
    "        patches = self.projection(embeddings)\n",
    "\n",
    "        patches = patches.view(batch_size, -1, self.in_channels, self.patch_size, self.patch_size, self.patch_size)\n",
    "\n",
    "        num_patches_x = x_dim // self.patch_size\n",
    "        num_patches_y = y_dim // self.patch_size\n",
    "        num_patches_z = z_dim // self.patch_size\n",
    "\n",
    "        patches = patches.view(batch_size, num_patches_x, num_patches_y, num_patches_z, self.in_channels, self.patch_size, self.patch_size, self.patch_size)\n",
    "\n",
    "        x = patches.permute(0, 4, 1, 5, 2, 6, 3, 7).contiguous()\n",
    "        x = x.view(batch_size, self.in_channels, x_dim, y_dim, z_dim)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = torch.rand(4, 32, 128, 128, 128)\n",
    "p = PatchEmbedding(8, 32,  512)\n",
    "r = ReversePatchEmbedding(8, 32, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = r(p(example), example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(example, out, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
